{"title":"Linear and Non-Linear Regression","markdown":{"yaml":{"title":"Linear and Non-Linear Regression"},"headingText":"Linear Regression","containsRefs":false,"markdown":"\n\n\n\nAndrew Zhang(azhang42)\n\n\n\n\nFirst, I will give a breif overview of linear regression. Linear regression is the process of fitting a linear line to a collection of data points. It can be given by the equation $$y=\\beta_0+\\beta_1x$$, where $\\beta_0$ and $\\beta_1$ are parameters that are optimized by the linear regression. The the \"goodness\" of the line is given by a cost function, such as least squares, which is simply the sum of the square between the line and the data points. The goal of linear regression is to minimize the cost function. $$ J(\\theta) = \\frac{1}{2m} \\sum_{i=1}^{m} (h_\\theta(x^{(i)}) - y^{(i)})^2$$\nWe can do this in two main ways. The first way is gradient descent, where we iteratively find the parameters with the lowest cost function. The second way is to use the normal equation, which is a closed form solution to the parameters that minimize the cost function.\n\n### Gradient descent\n$$\\theta_j := \\theta_j - \\alpha \\frac{\\partial}{\\partial \\theta_j} J(\\theta)$$\n\nAbove is the update rule for gradient descent, where $\\alpha$ is the learning rate and $\\theta$ is a parameter. THe partial derivative for the linear regression is \n\n$$\\frac{\\partial}{\\partial \\theta_j} J(\\theta) = \\frac{1}{m} \\sum_{i=1}^{m} ((h_\\theta(x^{(i)}) - y^{(i)}) \\cdot x_j^{(i)})$$\n\nWe update according along this gradient. Youc an image it like walking down a valley along the path of greatest decrease until you reach the bottom.\n\n### Normal Equation\n$$\\theta = (X^T X)^{-1} X^T y$$\n\nAbove is the normal equation or, which is the closed form solution to the parameters. This can be used to find the minimum of the cost function. This is a much faster way to find the parameters, but it is not always possible to use. If the matrix $X^T X$ is not invertible, then the normal equation cannot be used. Furthermore, it cannot be used for more complicated machine learning model such as neural networks.\n\nNow, we will go into a practical exmaple of linear regression. First, we will create a synthetic dataset and visualize it:\n\nNow, let's fit a linear regression model:\n\n### Non-Linear Regression\n\nNon-linear regression is the same thing as linear regression, except instead of fitting a linear line to the model, we fit a nonlinear line. We can use the same cost function and gradient descent algorithm as before, but we will need to change the hypothesis function. For example, we can use a quadratic function as the hypothesis function. $$y=\\beta_0+\\beta_1x+\\beta_2x^2$$\n","srcMarkdownNoYaml":"\n\n\n\nAndrew Zhang(azhang42)\n\n\n\n## Linear Regression\n\nFirst, I will give a breif overview of linear regression. Linear regression is the process of fitting a linear line to a collection of data points. It can be given by the equation $$y=\\beta_0+\\beta_1x$$, where $\\beta_0$ and $\\beta_1$ are parameters that are optimized by the linear regression. The the \"goodness\" of the line is given by a cost function, such as least squares, which is simply the sum of the square between the line and the data points. The goal of linear regression is to minimize the cost function. $$ J(\\theta) = \\frac{1}{2m} \\sum_{i=1}^{m} (h_\\theta(x^{(i)}) - y^{(i)})^2$$\nWe can do this in two main ways. The first way is gradient descent, where we iteratively find the parameters with the lowest cost function. The second way is to use the normal equation, which is a closed form solution to the parameters that minimize the cost function.\n\n### Gradient descent\n$$\\theta_j := \\theta_j - \\alpha \\frac{\\partial}{\\partial \\theta_j} J(\\theta)$$\n\nAbove is the update rule for gradient descent, where $\\alpha$ is the learning rate and $\\theta$ is a parameter. THe partial derivative for the linear regression is \n\n$$\\frac{\\partial}{\\partial \\theta_j} J(\\theta) = \\frac{1}{m} \\sum_{i=1}^{m} ((h_\\theta(x^{(i)}) - y^{(i)}) \\cdot x_j^{(i)})$$\n\nWe update according along this gradient. Youc an image it like walking down a valley along the path of greatest decrease until you reach the bottom.\n\n### Normal Equation\n$$\\theta = (X^T X)^{-1} X^T y$$\n\nAbove is the normal equation or, which is the closed form solution to the parameters. This can be used to find the minimum of the cost function. This is a much faster way to find the parameters, but it is not always possible to use. If the matrix $X^T X$ is not invertible, then the normal equation cannot be used. Furthermore, it cannot be used for more complicated machine learning model such as neural networks.\n\nNow, we will go into a practical exmaple of linear regression. First, we will create a synthetic dataset and visualize it:\n\nNow, let's fit a linear regression model:\n\n### Non-Linear Regression\n\nNon-linear regression is the same thing as linear regression, except instead of fitting a linear line to the model, we fit a nonlinear line. We can use the same cost function and gradient descent algorithm as before, but we will need to change the hypothesis function. For example, we can use a quadratic function as the hypothesis function. $$y=\\beta_0+\\beta_1x+\\beta_2x^2$$\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":false,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"jupyter"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true,"format-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["styles.css"],"output-file":"regression.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.3.450","theme":"cosmo","title":"Linear and Non-Linear Regression"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}